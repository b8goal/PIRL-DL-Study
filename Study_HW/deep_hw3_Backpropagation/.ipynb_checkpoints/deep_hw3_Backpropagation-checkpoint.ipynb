{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "  <br>\n",
    " \n",
    "신경망이 학습하는 것을 순서대로 나열해보면 input layer부터 각 layer를 지나가며 weight들을 곱하고 activation function(eg. sigmoid, ReLu)을 지나서 마지막 output layer에서 오차값을 계산한 이후 이를 뒤에서부터 순차로 돌아오며 오차값을 줄이는 방향으로 weight를 수정해나간다. 이를 위해서는 Backpropagation이라는 개념이 필요하다.  \n",
    "\n",
    "\n",
    "Backpropagation 이란 번역하자면 역전파법으로 보통 Back propagation of error의 약자로 보면 된다.  \n",
    "\n",
    "이름은 거창하지만 사실상 신경망의 마지막에 나오는 결과값의 오차에 각 변수가 얼마나 영향을 미치는지 알아보는 방법이라고 생각하면 된다.\n",
    "\n",
    "예를 들어보면 다음과 같은 식이 있다. \n",
    "\n",
    "### $$\\frac{\\partial L}{\\partial w_{11}} = {a}$$\n",
    "\n",
    "위 식이 의미하는 것은 무엇일까? L이 Loss라 생각하면 ${w_{11}}$의 가중치가 ${L}$에 미치는 영향이 ${a}$ 라는 의미이다.   \n",
    "다시말하면 ${w_{11}}$ 가 1 증가하면 L은 a만큼 증가한다는 뜻이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example1\n",
    "\n",
    "우선 다음 신경망 구조를 보자.\n",
    "\n",
    "<img src=\"./pic/img1.PNG\" width=80%/>\n",
    "\n",
    "흔히 보는 구조로 ${x_1}$ 과 ${w_1}$을 곱하고 마찬가지로 ${x_2}$ 과 ${w_2}$을 곱해서 서로 더해주는 구조이다.\n",
    "\n",
    "하지만, 이 게이트 방식의 네트워크는 계산시에는 굉장히 편하다. 하지만, 그리거나 보기에 이는 비효율적임을 알 수 있다.\n",
    "\n",
    "가령 ${x_1}$의 ${weight}$가 한개가 아니라 여러개라고 생각해 보자. 계산시에는 편리하지만 이를 표현하려면 매우 복잡해 질 것이다.\n",
    "\n",
    "따라서 다음과 같이 표기하여 그림에 편의를 주자.\n",
    "\n",
    "<img src=\"./pic/img2.PNG\" width=50%/>\n",
    "\n",
    "\n",
    "위 그림을 보면 그래프가 매우 간소해 짐을 알 수 있다. 그래프를 해석해보면 노드 ${x_1}$에 간선 ${w_1}$을 곱하여 각 화살표 끼리 더한 값은 ${u_1}$이다 라고 생각하면된다. 즉, 위의 그림과 완벽하게 일치하는 그림이다.\n",
    "\n",
    "조금더 의미적으로 해석하자면 ${x_1}$에 가중치 ${w_1}$을 준다고 해석하면 되겠다.\n",
    "\n",
    "이번엔 다음 그래프를 보자\n",
    "\n",
    "<img src=\"./pic/img3.PNG\" width=80%/>\n",
    "\n",
    "\n",
    "위 그래프에 무언가 많이 추가되었다. 오른쪽 길게 이어진 부분은 무엇일까?\n",
    "\n",
    "식으로 나타내 보면 다음과 같다. $$\\frac{1}{1+e^{-u_1}}$$\n",
    "\n",
    "그렇다! 길게 이어진 부분은 *sigmoid 함수이다. 이러한 함수를 길게 늘어쓰는건 비효율적이므로 다음과 같이 줄이자.\n",
    "\n",
    "\n",
    "##### *기억이 안난다면 Logistic Regression 부분을 복습하자\n",
    "\n",
    "<img src=\"./pic/img4.PNG\" width=50%/>\n",
    "\n",
    "위 그림은 ${u_1}$에서 sigmoid 함수 ${f}$를 적용해서 그 결과인 ${y_1}$을 만드는 그래프이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example2\n",
    "\n",
    "\n",
    "지금까지 배운 것을 이용하여 계산그래프를 축약해서 그럴듯한 신경망을 그려보면 다음과 같다.\n",
    "\n",
    "\n",
    "<img src=\"./pic/img5.PNG\" width=80%/>\n",
    "\n",
    "여기서 ${w^{(1)}_{11}}$의 의미는 ${x_1}$의 가중치이고 ${u_1}$으로 간다는 의미이며 1번째 layer라는 의미이다. 이는 후에 행렬로 표현되면 인덱스가 되기도 한다.\n",
    "\n",
    "${L}$ 은 ${y_{1}}$에 ${w^{(2)}_{11}}$(제곱이 아니다) 을 곱하고 ${y_{2}}$에 ${w^{(2)}_{21}}$ 을 곱해서 더한 값과 같다. \n",
    "\n",
    "우리가 알고싶은 값들은 다음과 같다. \n",
    "\n",
    "### $\\frac{\\partial L}{\\partial w^{(1)}_{11}}_, \\frac{\\partial L}{\\partial w^{(1)}_{12}}_, \\frac{\\partial L}{\\partial w^{(1)}_{13}}_,  \\frac{\\partial L}{\\partial w^{(1)}_{14}}_, \\frac{\\partial L}{\\partial w^{(1)}_{21}} _,\\frac{\\partial L}{\\partial w^{(1)}_{22}}_, {...} \\frac{\\partial L}{\\partial w^{(1)}_{44}} _, \\frac{\\partial L}{\\partial w^{(2)}_{11}}_,  \\frac{\\partial L}{\\partial w^{(2)}_{21}} $\n",
    "\n",
    "과 같은 식이다. 즉, 모든 신경망의 가중치들의 편미분값을 구하여 해당 가중치를 업데이트 하여 오차를 줄이는 것이다. \n",
    "\n",
    "우선 수식적으로 이를 이해하고 계산 그래프로 올라와보자.\n",
    "<br>\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w^{(1)}_{11}} 를  계산해보자 $$\n",
    "\n",
    "\n",
    "우선 만약 앞에서 부터 chain rule을 적용해가며 진행하면 다음과 같다.\n",
    "<br><br>\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w^{(1)}_{11}} = \\frac{\\partial L}{\\partial y_1}\\frac{\\partial y_1}{\\partial u_{1}}\\frac{\\partial u_1}{\\partial w^{(1)}_{11}}$$\n",
    "\n",
    "마찬가지로 다음과 같다. \n",
    "<br><br>\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w^{(1)}_{21}} =  \\frac{\\partial L}{\\partial y_1}\\frac{\\partial y_1}{\\partial u_1}\\frac{\\partial u_1}{\\partial w^{(1)}_{21}}$$\n",
    "\n",
    "<br><br>\n",
    "식을 잘 살펴보면 다음 부분에 중복이 일어난다.<font size=\"4\"> $\\frac{\\partial L}{\\partial y_1}\\frac{\\partial y_1}{\\partial u_1}$ </font>\n",
    "\n",
    "이러한 중복을 제거하지 않고 계산하다보면 계산 시간이 굉장히 오래걸리게 된다. 그 외에도 위 방식은 layer가 아주 적음에도 식이 복잡해지기 시작한다. 후에 다층 layer가 나오게 되면 더욱 복잡해 질 것이다. 또한 코딩으로 미분을 하기는 쉽지 않다.\n",
    "\n",
    "위 단점을 피하기 위해 나온 방법이 Backpropagation이다.\n",
    "\n",
    "Backpropagation의 장점은 아래와 같다.\n",
    "\n",
    "> 1) 편미분 계산에 중복 계산을 피할 수 있다.  \n",
    "> 2) 전체를 보지 않고 자기 자신만 계산하면 된다.   \n",
    "> 3) 게이트별 분할이 되어있어 미분값을 쉽게 구할 수 있다. \n",
    "\n",
    "##### 참고) 위 특징1,2는 Dynamic programming의 bottom up fashion 특징이다.\n",
    "\n",
    "앞으로 우리는 계산그래프를 통해 간단한 Backpropagation을 구현해보자\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem1\n",
    "\n",
    "### 덧셈 게이트 구현해보기\n",
    "\n",
    "간단한 덧셈노드를 파이썬으로 구현해보고자 한다. 다시 간단한 계산 그래프로 돌아가보자\n",
    "\n",
    "우선 다음과 같은 순전파(forward propagation) 덧셈노드가 있다.\n",
    "\n",
    "\n",
    "<img src=\"./pic/img6.PNG\" width=70%/>\n",
    "\n",
    "앞에서부터 어떠한 계산이 있던 현재 빨간 선으로 들어오는 값은 a이므로 이전은 생각할 필요 없다.\n",
    "\n",
    "마찬가지로 파란 화살표도 동일하다. 위 두 값을 더한 결과를 t라 하고 그 이후는 생각히자 않는다.\n",
    "\n",
    "\n",
    "다음은 위 그림의 역전파(backward propagation)계산그래프 이다. \n",
    "\n",
    "<img src=\"./pic/img7.PNG\" width=70%/>\n",
    "\n",
    "처음 <font size=\"4\"> $\\frac{\\partial {L}}{\\partial {t}}$ </font>를 보면 뒤쪽에서 이미 계산이 된 값이라고 생각한다. 다시말하면 위 그림에 나오지 않은 뒤에 노드에서 계산을하여 넘겨준 값이라고 생각하면 된다.\n",
    "\n",
    "이러한 stream을 계속 역으로 타고 올라간다. 다음 구하고자 하는 값은 <font size=\"4\"> $\\frac{\\partial {L}}{\\partial {a}}$ </font>이므로 chain rule를 이용하여 계산해보면 <font size=\"4\"> $\\frac{\\partial {t}}{\\partial {a}}$ </font> = 1 이므로 값의 변화가 없게 된다.\n",
    "\n",
    "아래쪽도 마찬가지이다. 이러한 값이 변하지 않고 앞의 gradient를 그대로 전달하는 add gate를 gradient distributor라고 한다. \n",
    "\n",
    "그러면 이제 드디어 AddGate를 만들어 보자\n",
    "\n",
    "우리가 만들 AddGate의 역할은 다음과 같다.\n",
    "\n",
    "> 1. 두개의 들어온 값으로 순전파를 만든다.\n",
    "2. 하나의 들어온 값으로 역전파를 만들어 분배한다.\n",
    "\n",
    "\n",
    "##### 참고 \n",
    "upstream gradient: 뒤쪽에서 계산되어 들어오는 gradient값이다. 위 그림에서는 <font size=\"4\"> $\\frac{\\partial {L}}{\\partial {t}}$ </font><br>\n",
    "local gradient: 현재 자신의 gradient값. 위 그림에서는 <font size=\"4\"> $\\frac{\\partial {t}}{\\partial {a}}_,\\frac{\\partial {t}}{\\partial {b}}$ </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward 와 backward 함수를 완성하자\n",
    "\n",
    "\n",
    "class AddGate:\n",
    "    def __init__(self):\n",
    "        self.a = None\n",
    "        self.b = None\n",
    "        \n",
    "    def forward(self, a, b):\n",
    "        \n",
    "    \n",
    "    def backward(self, up_grad):\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem2\n",
    "\n",
    "### 곱셈 게이트 구현해보기\n",
    "\n",
    "덧셈노드와 마찬가지로 아래 그림을 참고하여 곱셈도 순전파와 역전파를 구현하는 class를 만들어보자.\n",
    "\n",
    "다음은 순전파 그래프이다. \n",
    "<img src=\"./pic/img8.PNG\" width=70%/>\n",
    "\n",
    "\n",
    "다음은 역전파 그래프이다. \n",
    "<img src=\"./pic/img9.PNG\" width=70%/>\n",
    "\n",
    "역전파의 특징은 각 local gradient 값이 반대 방향의 순전파 값과 같다.\n",
    "\n",
    "이러한 특징 떄문에 곱셈 게이트는 gradient switcher라고도 부른다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward 와 backward 함수를 완성하자\n",
    "\n",
    "class MultiplyGate:\n",
    "    def __init__(self):\n",
    "        self.a = None\n",
    "        self.b = None\n",
    "        \n",
    "    def forward(self, a, b):\n",
    "        \n",
    "    \n",
    "    def backward(self, up_grad):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem3\n",
    "\n",
    "### 순전파, 역전파 테스트하기\n",
    "\n",
    "우리가 만든 곱셈, 덧셈 게이트를 이용하여 실제로 곱셈과 덧셈으로 이루어진 식의 gradient값을 구할 수 있는지 테스트 해보자.\n",
    "\n",
    "<img src=\"./pic/img10.PNG\" width=85%/>\n",
    "\n",
    "위 그래프는 다음과 같은 식을 나타내는 값이다. \n",
    "\n",
    "$$ f(x,y,z) = (x+y)z   $$\n",
    "\n",
    "위 함수에서 우리가 최종적으로 구하고 싶은 값은 다음과 같다.\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x}_, \\frac{\\partial f}{\\partial y}_,\\frac{\\partial f}{\\partial z}$$\n",
    "\n",
    "아주 간단한 함수이다보니 오히려 실제로는 수학적 계산을 통해 값을 구하는게 빠를것이다. 하지만 우리는 코딩으로 풀어보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "x = -2; y = 5; z = -4\n",
    "\n",
    "# addgate와 mulgate를 만들어 위 그래프와 같은 흐름을 보내보자\n",
    "# 최종 결과값은 -12가 나오면 된다.\n",
    "\n",
    "# 객체생성\n",
    "\n",
    "\n",
    "# forward propagation\n",
    "\n",
    "\n",
    "\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backpropagation을 보면 실제로 다음과 같이 동작한다.\n",
    "\n",
    "즉 x의 값이 1 증가하면 f의 값은 -4만큼 영향을 받는다. \n",
    "\n",
    "아래와 같은 값을 위에서 구한 forward에 이어서 직접 구현해서 계산해보자.\n",
    "\n",
    "<img src=\"./pic/img11.PNG\" width=95%/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_grad:  -4\n",
      "y_grad:  -4\n",
      "z_grad:  3\n"
     ]
    }
   ],
   "source": [
    "# 시작하는 미분값은 1이다.\n",
    "f_grad = 1 \n",
    "\n",
    "# backward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"x_grad: \", x_grad)\n",
    "print(\"y_grad: \", y_grad)\n",
    "print(\"z_grad: \", z_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem4\n",
    "\n",
    "### sigmoid 구현하기\n",
    "\n",
    "sigmoid 함수는 다음과 같은 그래프를 가진다\n",
    "\n",
    "<img src=\"./pic/img12.PNG\" width=70%/>\n",
    "\n",
    "위 그림을 하나하나 직접 구현하여 sigmoid gate를 만드는것은 매우 비효율적이다.\n",
    "\n",
    "아래 sigmoid 수식을 한번 직접 미분해보고 적당히 변형시켜 식을 단순화하자.\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "\n",
    "계산은 생략하고 최종 결과만 보면 다음과 같다.  (모두 스스로 해보길 바란다.)\n",
    "\n",
    "$$\\frac{\\partial \\sigma(x)}{\\partial x} = (1-\\sigma(x))\\sigma(x)$$\n",
    "\n",
    "정말 고맙게도 식이 간소화 되었다. 위 식을 이용해서 sigmoid gate를 구현해보자\n",
    "\n",
    "* exp 계산시 numpy모듈을 이용하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SigmoidGate:\n",
    "    def __init__(self):\n",
    "        self.sigmoid = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "    \n",
    "    def backward(self, up_grad):\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU 구현해보기\n",
    "\n",
    "\n",
    "ReLU 의 식은 이렇게 정의할 수 있다. \n",
    "\n",
    "$$ ReLU(x) = max(0,x) $$ \n",
    "\n",
    "즉 max gate와 같다. 위 식을 이용해서 직접 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUGate:\n",
    "    def __init__(self):\n",
    "        self.ReLU = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "       \n",
    "    \n",
    "    def backward(self, up_grad):\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 해보기\n",
    "\n",
    "위에서 만든 sigmoid게이트를 이용해서 다음 그래프를 테스트해보자\n",
    "\n",
    "<img src=\"./pic/img14.PNG\" width=100%/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------forward----------------------\n",
      "y3: 0.73\n",
      "\n",
      "---------------------backward---------------------\n",
      "w0_grad: -0.2\n",
      "x0_grad: 0.4\n",
      "w1_grad: -0.4\n",
      "x1_grad: -0.6\n",
      "w2_grad: 0.2\n"
     ]
    }
   ],
   "source": [
    "w0 = 2.0; x0 = -1.0; w1 = -3.0; x1 = -2.0; w2 = -3.0;\n",
    "\n",
    "# 객체 생성\n",
    "\n",
    "\n",
    "# forward \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"{0:-^50}\".format(\"forward\"))\n",
    "print(\"y3:\", y3.round(2))\n",
    "print()\n",
    "\n",
    "y3_grad = 1\n",
    "# backward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"{0:-^50}\".format(\"backward\"))\n",
    "print(\"w0_grad:\", w0_grad.round(1))\n",
    "print(\"x0_grad:\", x0_grad.round(1))\n",
    "print(\"w1_grad:\", w1_grad.round(1))\n",
    "print(\"x1_grad:\", x1_grad.round(1))\n",
    "print(\"w2_grad:\", w2_grad.round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트해보기2\n",
    "\n",
    "다음엔 ReLU로 테스트해보자\n",
    "\n",
    "<img src=\"./pic/img15.PNG\" width=90%/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------forward----------------------\n",
      "y3: 1.0\n",
      "\n",
      "---------------------backward---------------------\n",
      "w0_grad: -1.0\n",
      "x0_grad: 2.0\n",
      "w1_grad: -2.0\n",
      "x1_grad: -3.0\n",
      "w2_grad: 1\n"
     ]
    }
   ],
   "source": [
    "w0 = 2.0; x0 = -1.0; w1 = -3.0; x1 = -2.0; w2 = -3.0;\n",
    "\n",
    "# 객체 생성\n",
    "mul_gate1 = MultiplyGate()\n",
    "mul_gate2 = MultiplyGate()\n",
    "add_gate1 = AddGate() \n",
    "add_gate2 = AddGate()\n",
    "relu_gate = ReLUGate()\n",
    "\n",
    "# forward \n",
    "y0 = mul_gate1.forward(w0, x0)\n",
    "y1 = mul_gate2.forward(w1, x1)\n",
    "u0 = add_gate1.forward(y0, y1)\n",
    "u1 = add_gate2.forward(u0, w2)\n",
    "y3 = relu_gate.forward(u1)\n",
    "\n",
    "print(\"{0:-^50}\".format(\"forward\"))\n",
    "print(\"y3:\", y3)\n",
    "print()\n",
    "\n",
    "# backward\n",
    "y3_grad = 1\n",
    "u1_grad = relu_gate.backward(y3_grad)\n",
    "u0_grad, w2_grad = add_gate2.backward(u1_grad)\n",
    "y0_grad, y1_grad = add_gate1.backward(u0_grad)\n",
    "w0_grad, x0_grad = mul_gate1.backward(y0_grad)\n",
    "w1_grad, x1_grad = mul_gate2.backward(y1_grad)\n",
    "\n",
    "print(\"{0:-^50}\".format(\"backward\"))\n",
    "print(\"w0_grad:\", w0_grad)\n",
    "print(\"x0_grad:\", x0_grad)\n",
    "print(\"w1_grad:\", w1_grad)\n",
    "print(\"x1_grad:\", x1_grad)\n",
    "print(\"w2_grad:\", w2_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
